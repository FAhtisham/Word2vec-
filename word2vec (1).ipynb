{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r2tm303wwgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZZT47OAya_M",
        "colab_type": "text"
      },
      "source": [
        "# **Word2vec**\n",
        " generates separate vector for each of the word present in the corpus. For example, the word **“happy”** can be represented as a vector of 4 dimensions **[0.24, 0.45, 0.11, 0.49]** and **“sad”** has a vector of **[0.88, 0.78, 0.45, 0.91]**\n",
        "\n",
        "\n",
        "whole process = **word embedding**, **motive** = TO generate vectors so that linear algebra operations can be performed on numbers.\n",
        "\n",
        "\n",
        "# Types\n",
        "- CBOW:\n",
        "       several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
        "- Skipgram:\n",
        "           works well with small amount of the training data, represents well even rare words or phrases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/800/0*o2FCVrLKtdcxPQqc.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n99RAVgw1tlP",
        "colab_type": "text"
      },
      "source": [
        "# **Implementation** (Skipgram)\n",
        "There are six steps that will be followed till the completion of the work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_LSUrzI2CQV",
        "colab_type": "text"
      },
      "source": [
        "* 1. **Data preparation**\n",
        "   In reality, text data are unstructured and can be “dirty”. Cleaning them will involve steps such as removing stop words, punctuations, convert text to lowercase (actually depends on your use-case), replacing digits, etc\n",
        "                               "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXcdvyWS1xWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d096009-6575-4c06-a5b7-ea0ac4f0c7ed"
      },
      "source": [
        "# Initial corpus\n",
        "text = \"I want to be a genomic data scientist here in TU Kaiserslautern under MEC\"\n",
        "\n",
        "\n",
        "# Preprocessing the corpus\n",
        "corpus = [[word.lower() for word in text.split()]]\n",
        "print(\"Corpus is:\", corpus)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus is: [['i', 'want', 'to', 'be', 'a', 'genomic', 'data', 'scientist', 'here', 'in', 'tu', 'kaiserslautern', 'under', 'mec']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bl9S9iE6p-M",
        "colab_type": "text"
      },
      "source": [
        "* 2. **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzdGziYJ2z8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary for the hyperparameters\n",
        "hyperparameters= {'window_size': 2,\n",
        "                  'embedding_size': 14,\n",
        "                  'lr': 0.01,\n",
        "                  'epochs': 300\n",
        "                  }"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIO-VnJl4kB9",
        "colab_type": "text"
      },
      "source": [
        "**Sliding window**\n",
        "  context words are words that are neighbouring the target word\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/560/1*tD7P83Bl7dB91iNwYHEmEg.png\">\n",
        "\n",
        "\n",
        "**[n]** This is the dimension of the word embedding and it typically ranges from 100 to 300 depending on your vocabulary size. Dimension size beyond 300 tends to have diminishing benefit. Do note that the dimension is also the size of the hidden layer.\n",
        "\n",
        "**[epochs]** This is the number of training epochs. In each epoch, we cycle through all training samples.\n",
        "\n",
        "**[learning_rate]** The learning rate controls the amount of adjustment made to the weights with respect to the loss gradient\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "* 3. **Generation of Training Data**\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/800/1*vunPUSipHyot3vvwLcND_w.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2XwA9Vz3Ima",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uGOtayk4Lv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class word2vec():\n",
        "  def __init__(self):\n",
        "    self.window = hyperparameters['window_size']\n",
        "    self.epochs = hyperparameters['epochs']\n",
        "    self.learning_rate = hyperparameters['lr']\n",
        "    self.embedding_size = hyperparameters['embedding_size']\n",
        "\n",
        "\n",
        "  def generate_training_data(self, hyperparameters, corpus):\n",
        "   \n",
        "    # Find unique words using dictionary\n",
        "    word_count = defaultdict(int)\n",
        "    \n",
        "    # We have only one row here and we have approximately 14 words in it\n",
        "    for row in corpus:\n",
        "      for word in row:\n",
        "        word_count[word]+=1\n",
        "    \n",
        "    # Unique words\n",
        "    self.u_counts = len(word_count.keys())\n",
        "    print(\"Unique words = \", self.u_counts)\n",
        "\n",
        "    # Creating a lookup dictionary or vocabulary\n",
        "    self.words_list = list(word_count.keys())\n",
        "    #print(self.words_list)\n",
        "\n",
        "\n",
        "    # Generate word:index\n",
        "    self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
        "    #print(\"Generated word:index\", self.word_index)\n",
        "\n",
        "    # Generate index:word\n",
        "    self.index_word =dict((i, word) for i, word in enumerate(self.words_list))\n",
        "    #print(\"Generated word:index\", self.index_word)\n",
        "\n",
        "\n",
        "    training_data=[]\n",
        "\n",
        "    # Cycle through each sentence in corpus\n",
        "    for sentence in corpus:\n",
        "      sent_len = len(sentence)\n",
        "\n",
        "      # Cycle through each word in sentence\n",
        "      for i, word in enumerate(sentence):\n",
        "        # print(\"Petrit: \",i, word)\n",
        "        # Convert target word to one-hot\n",
        "        w_target = self.word2onehot(sentence[i])\n",
        "        print(\"One hot encoded vectors: \", w_target)\n",
        "        w_context = []\n",
        "\n",
        "        # getting the context \n",
        "        for j in range(i - self.window, i + self.window + 1): \n",
        "          # print(\"Line for petrit = \", i-self.window, i+self.window)       \n",
        "          # Criteria for context word \n",
        "          # 1. Target word cannot be context word (j != i)\n",
        "          # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
        "          # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
        "\n",
        "          if j != i and j <= sent_len-1 and j >= 0:\n",
        "\n",
        "            # Append the one-hot representation of word to w_context\n",
        "            w_context.append(self.word2onehot(sentence[j]))\n",
        "\n",
        "            # print(sentence[i], sentence[j]) \n",
        "            # training_data contains a one-hot representation of the target word and context words\n",
        "        training_data.append([w_target, w_context])\n",
        "    return np.array(training_data)\n",
        "\n",
        "\n",
        "\n",
        "  def word2onehot(self, word):\n",
        "\n",
        "    # Initializing one hot vector with 14 values\n",
        "    word_vec = [0 for i in range (0, self.u_counts)]\n",
        "    #print(word_vec)\n",
        "\n",
        "    # Now we have to get the index of the word\n",
        "    word_index = self.word_index[word]\n",
        "\n",
        "    # Change the value to 1 according to the index\n",
        "    word_vec[word_index] = 1\n",
        "\n",
        "    return word_vec\n",
        "\n",
        "  def initialize_weights(self, training_data):\n",
        "      self.w1  = np.random.uniform(-1,1, (self.u_counts, self.embedding_size))\n",
        "      self.w2 = np.random.uniform(-1, 1, (self.embedding_size, self.u_counts))\n",
        "\n",
        "  def train(self, training_data):\n",
        "    losses=[]\n",
        "    count=0\n",
        "    for i in range (self.epochs):\n",
        "      # Initialize loss to 0 at the start of every epoch\n",
        "      \n",
        "      self.loss = 0\n",
        "      \n",
        "      # Forward pass\n",
        "      for w_target, w_context in training_data:\n",
        "\n",
        "        count+=1\n",
        "\n",
        "        # print(w_target, \"and \", w_context)\n",
        "        # 1. predicted y using softmax (y_pred) 2. matrix of hidden layer (h) 3. output layer before softmax (u)\n",
        "        pred, hidden, s_wio_activation = self.feed_forward_pass(w_target)\n",
        "        \n",
        "        # Backward Pass\n",
        "        # Calculate error\n",
        "        # 1. For a target word, calculate difference between y_pred and each of the context words\n",
        "        # 2. Sum up the differences using np.sum to give us the error for this particular target word\n",
        "\n",
        "\n",
        "        EI = np.sum([np.subtract(pred, word) for word in w_context], axis=0)\n",
        "        \n",
        "\n",
        "\n",
        "        #print(\"The error value is:\", EI, \"count = \", count)\n",
        "        # Backpropagation\n",
        "        # We use SGD to backpropagate errors - calculate loss on the output layer \n",
        "\n",
        "\n",
        "        self.backpropagation(EI, hidden, w_target)\n",
        "        #losses.append(self.loss)\n",
        "        # Calculate loss\n",
        "        # There are 2 parts to the loss function\n",
        "        # Part 1: -ve sum of all the output +\n",
        "        # Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)\n",
        "        # Note: word.index(1) returns the index in the context word vector with value 1\n",
        "        # Note: u[word.index(1)] returns the value of the output layer before softmax\n",
        "        \n",
        "        self.loss += -np.sum([s_wio_activation[word.index(1)] for word in w_context]) + len(w_context) * np.log(np.sum(np.exp(s_wio_activation)))\n",
        "        \n",
        "      print('Epoch:', i, \"Loss:\", self.loss)\n",
        "\n",
        "\n",
        "\n",
        "  def backpropagation(self, e, h, x):\n",
        "    # https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html\n",
        "    # Column vector EI represents row-wise sum of prediction errors across each context word for the current center word\n",
        "    # Going backwards, we need to take derivative of E with respect of w2\n",
        "    # h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9\n",
        "\n",
        "    dl_dw2 = np.outer(h, e)\n",
        "    #print(\"H SHape\", h.shape, \"e\", e.shape, \"dl_w2\", dl_dw2.shape )\n",
        "    # x - shape 1x8, w2 - 5x8, e.T - 8x1\n",
        "    # x - 1x8, np.dot() - 5x1, dl_dw1 - 8x5\n",
        "    dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
        "\n",
        "    # Update weights\n",
        "    self.w1 = self.w1 - (self.learning_rate * dl_dw1)\n",
        "    self.w2 = self.w2 - (self.learning_rate * dl_dw2)\n",
        "    \n",
        "  def feed_forward_pass(self, x):\n",
        "    #  WHY T?\n",
        "    # Getting the hidden layer without any activation function\n",
        "    h = np.dot(self.w1.T, x)\n",
        "\n",
        "    # Getting Output layer without activation\n",
        "    u = np.dot(self.w2.T, h )\n",
        "\n",
        "    # Passing output through softmax\n",
        "    y_context = self.softmax(u)\n",
        "\n",
        "    return y_context, h, u\n",
        "\n",
        "  def softmax(self, x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "  \n",
        "  def backp_error_loss(self, training_loss):\n",
        "\n",
        "    for i in range (self.epochs):\n",
        "      self.loss\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Sn_4g_Rn22",
        "colab_type": "text"
      },
      "source": [
        "* 4. **Model Training**\n",
        "\n",
        "skipgram architecture presented in the diagram\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/560/1*uuVrJhSF89KGJPltvJ4xhg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IofL5julYiNW",
        "colab_type": "text"
      },
      "source": [
        "**Forward pass of the training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AqK2a9NReJi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5349f1b-5440-433d-f7b3-ef5258fc36c7"
      },
      "source": [
        "# Initialize the class object\n",
        "wvec = word2vec()\n",
        "\n",
        "# Pass the arguments and get the ndarray of one hot encoded vectors\n",
        "training_data = wvec.generate_training_data(hyperparameters, corpus)\n",
        "wvec.initialize_weights(training_data)\n",
        "wvec.train(training_data)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words =  14\n",
            "One hot encoded vectors:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "One hot encoded vectors:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "Epoch: 0 Loss: 168.69164553066898\n",
            "Epoch: 1 Loss: 160.76474513611691\n",
            "Epoch: 2 Loss: 153.9962833866621\n",
            "Epoch: 3 Loss: 148.1490963687942\n",
            "Epoch: 4 Loss: 143.0381147433184\n",
            "Epoch: 5 Loss: 138.5203887440734\n",
            "Epoch: 6 Loss: 134.4863360570097\n",
            "Epoch: 7 Loss: 130.8518427178316\n",
            "Epoch: 8 Loss: 127.55182364517202\n",
            "Epoch: 9 Loss: 124.53533992852248\n",
            "Epoch: 10 Loss: 121.7620225396519\n",
            "Epoch: 11 Loss: 119.19947990773234\n",
            "Epoch: 12 Loss: 116.82141332780391\n",
            "Epoch: 13 Loss: 114.60623408666409\n",
            "Epoch: 14 Loss: 112.53603634132047\n",
            "Epoch: 15 Loss: 110.59582421785466\n",
            "Epoch: 16 Loss: 108.77292263110849\n",
            "Epoch: 17 Loss: 107.05652259966375\n",
            "Epoch: 18 Loss: 105.43732638626896\n",
            "Epoch: 19 Loss: 103.90726780528827\n",
            "Epoch: 20 Loss: 102.45928997643686\n",
            "Epoch: 21 Loss: 101.08716764498284\n",
            "Epoch: 22 Loss: 99.78536458246307\n",
            "Epoch: 23 Loss: 98.54891896647027\n",
            "Epoch: 24 Loss: 97.37335131431144\n",
            "Epoch: 25 Loss: 96.25459072553927\n",
            "Epoch: 26 Loss: 95.18891602535741\n",
            "Epoch: 27 Loss: 94.17290900594196\n",
            "Epoch: 28 Loss: 93.2034174162223\n",
            "Epoch: 29 Loss: 92.27752570908306\n",
            "Epoch: 30 Loss: 91.39253185570935\n",
            "Epoch: 31 Loss: 90.545928802465\n",
            "Epoch: 32 Loss: 89.7353893877872\n",
            "Epoch: 33 Loss: 88.9587537591421\n",
            "Epoch: 34 Loss: 88.21401853279943\n",
            "Epoch: 35 Loss: 87.49932711989544\n",
            "Epoch: 36 Loss: 86.81296079872548\n",
            "Epoch: 37 Loss: 86.15333024418382\n",
            "Epoch: 38 Loss: 85.51896733086575\n",
            "Epoch: 39 Loss: 84.90851710803759\n",
            "Epoch: 40 Loss: 84.32072990496613\n",
            "Epoch: 41 Loss: 83.75445356711106\n",
            "Epoch: 42 Loss: 83.20862585076351\n",
            "Epoch: 43 Loss: 82.68226701909666\n",
            "Epoch: 44 Loss: 82.17447268918825\n",
            "Epoch: 45 Loss: 81.68440697984022\n",
            "Epoch: 46 Loss: 81.21129600595349\n",
            "Epoch: 47 Loss: 80.75442175835913\n",
            "Epoch: 48 Loss: 80.31311639953073\n",
            "Epoch: 49 Loss: 79.88675699636478\n",
            "Epoch: 50 Loss: 79.4747607018445\n",
            "Epoch: 51 Loss: 79.07658038835056\n",
            "Epoch: 52 Loss: 78.6917007269847\n",
            "Epoch: 53 Loss: 78.31963469976694\n",
            "Epoch: 54 Loss: 77.95992052512797\n",
            "Epoch: 55 Loss: 77.61211897185517\n",
            "Epoch: 56 Loss: 77.27581103262125\n",
            "Epoch: 57 Loss: 76.95059592543943\n",
            "Epoch: 58 Loss: 76.63608938979972\n",
            "Epoch: 59 Loss: 76.33192224376951\n",
            "Epoch: 60 Loss: 76.03773916885716\n",
            "Epoch: 61 Loss: 75.75319769079252\n",
            "Epoch: 62 Loss: 75.4779673264\n",
            "Epoch: 63 Loss: 75.21172886924998\n",
            "Epoch: 64 Loss: 74.9541737895987\n",
            "Epoch: 65 Loss: 74.70500372710501\n",
            "Epoch: 66 Loss: 74.46393005780095\n",
            "Epoch: 67 Loss: 74.23067351967701\n",
            "Epoch: 68 Loss: 74.00496388393451\n",
            "Epoch: 69 Loss: 73.78653966139201\n",
            "Epoch: 70 Loss: 73.5751478356789\n",
            "Epoch: 71 Loss: 73.37054361668787\n",
            "Epoch: 72 Loss: 73.1724902092989\n",
            "Epoch: 73 Loss: 72.9807585936433\n",
            "Epoch: 74 Loss: 72.79512731417773\n",
            "Epoch: 75 Loss: 72.61538227561613\n",
            "Epoch: 76 Loss: 72.44131654435634\n",
            "Epoch: 77 Loss: 72.27273015447277\n",
            "Epoch: 78 Loss: 72.10942991765674\n",
            "Epoch: 79 Loss: 71.95122923670343\n",
            "Epoch: 80 Loss: 71.7979479222887\n",
            "Epoch: 81 Loss: 71.64941201287441\n",
            "Epoch: 82 Loss: 71.50545359763846\n",
            "Epoch: 83 Loss: 71.36591064236417\n",
            "Epoch: 84 Loss: 71.23062681824234\n",
            "Epoch: 85 Loss: 71.09945133355528\n",
            "Epoch: 86 Loss: 70.97223876821923\n",
            "Epoch: 87 Loss: 70.84884891117102\n",
            "Epoch: 88 Loss: 70.72914660059058\n",
            "Epoch: 89 Loss: 70.61300156695897\n",
            "Epoch: 90 Loss: 70.50028827895933\n",
            "Epoch: 91 Loss: 70.39088579223512\n",
            "Epoch: 92 Loss: 70.2846776010276\n",
            "Epoch: 93 Loss: 70.18155149272059\n",
            "Epoch: 94 Loss: 70.08139940532543\n",
            "Epoch: 95 Loss: 69.98411728794339\n",
            "Epoch: 96 Loss: 69.88960496424384\n",
            "Epoch: 97 Loss: 69.79776599899853\n",
            "Epoch: 98 Loss: 69.70850756770993\n",
            "Epoch: 99 Loss: 69.62174032936997\n",
            "Epoch: 100 Loss: 69.53737830238116\n",
            "Epoch: 101 Loss: 69.45533874366822\n",
            "Epoch: 102 Loss: 69.37554203100068\n",
            "Epoch: 103 Loss: 69.29791154854294\n",
            "Epoch: 104 Loss: 69.22237357563868\n",
            "Epoch: 105 Loss: 69.14885717883134\n",
            "Epoch: 106 Loss: 69.07729410711394\n",
            "Epoch: 107 Loss: 69.00761869039415\n",
            "Epoch: 108 Loss: 68.93976774115404\n",
            "Epoch: 109 Loss: 68.8736804592763\n",
            "Epoch: 110 Loss: 68.80929834000254\n",
            "Epoch: 111 Loss: 68.74656508498335\n",
            "Epoch: 112 Loss: 68.68542651637367\n",
            "Epoch: 113 Loss: 68.62583049392288\n",
            "Epoch: 114 Loss: 68.56772683500314\n",
            "Epoch: 115 Loss: 68.5110672375168\n",
            "Epoch: 116 Loss: 68.4558052056193\n",
            "Epoch: 117 Loss: 68.4018959781917\n",
            "Epoch: 118 Loss: 68.34929645999398\n",
            "Epoch: 119 Loss: 68.29796515542849\n",
            "Epoch: 120 Loss: 68.24786210484159\n",
            "Epoch: 121 Loss: 68.1989488232904\n",
            "Epoch: 122 Loss: 68.1511882416999\n",
            "Epoch: 123 Loss: 68.10454465033752\n",
            "Epoch: 124 Loss: 68.05898364452949\n",
            "Epoch: 125 Loss: 68.01447207254576\n",
            "Epoch: 126 Loss: 67.97097798557972\n",
            "Epoch: 127 Loss: 67.92847058974995\n",
            "Epoch: 128 Loss: 67.88692020005226\n",
            "Epoch: 129 Loss: 67.84629819619143\n",
            "Epoch: 130 Loss: 67.80657698022316\n",
            "Epoch: 131 Loss: 67.76772993593839\n",
            "Epoch: 132 Loss: 67.72973138992391\n",
            "Epoch: 133 Loss: 67.69255657423386\n",
            "Epoch: 134 Loss: 67.65618159060976\n",
            "Epoch: 135 Loss: 67.62058337618748\n",
            "Epoch: 136 Loss: 67.58573967063157\n",
            "Epoch: 137 Loss: 67.55162898463942\n",
            "Epoch: 138 Loss: 67.51823056975952\n",
            "Epoch: 139 Loss: 67.48552438946969\n",
            "Epoch: 140 Loss: 67.45349109146298\n",
            "Epoch: 141 Loss: 67.42211198109138\n",
            "Epoch: 142 Loss: 67.39136899591844\n",
            "Epoch: 143 Loss: 67.36124468133418\n",
            "Epoch: 144 Loss: 67.33172216718756\n",
            "Epoch: 145 Loss: 67.3027851453928\n",
            "Epoch: 146 Loss: 67.27441784846846\n",
            "Epoch: 147 Loss: 67.24660502896909\n",
            "Epoch: 148 Loss: 67.21933193977131\n",
            "Epoch: 149 Loss: 67.19258431517734\n",
            "Epoch: 150 Loss: 67.16634835280091\n",
            "Epoch: 151 Loss: 67.14061069620166\n",
            "Epoch: 152 Loss: 67.1153584182359\n",
            "Epoch: 153 Loss: 67.09057900509221\n",
            "Epoch: 154 Loss: 67.06626034098278\n",
            "Epoch: 155 Loss: 67.04239069346163\n",
            "Epoch: 156 Loss: 67.01895869934278\n",
            "Epoch: 157 Loss: 66.99595335119234\n",
            "Epoch: 158 Loss: 66.97336398436937\n",
            "Epoch: 159 Loss: 66.95118026459215\n",
            "Epoch: 160 Loss: 66.92939217600671\n",
            "Epoch: 161 Loss: 66.90799000973624\n",
            "Epoch: 162 Loss: 66.88696435289026\n",
            "Epoch: 163 Loss: 66.86630607801398\n",
            "Epoch: 164 Loss: 66.8460063329587\n",
            "Epoch: 165 Loss: 66.82605653115513\n",
            "Epoch: 166 Loss: 66.80644834227256\n",
            "Epoch: 167 Loss: 66.78717368324669\n",
            "Epoch: 168 Loss: 66.76822470966133\n",
            "Epoch: 169 Loss: 66.74959380746768\n",
            "Epoch: 170 Loss: 66.73127358502786\n",
            "Epoch: 171 Loss: 66.7132568654681\n",
            "Epoch: 172 Loss: 66.69553667932895\n",
            "Epoch: 173 Loss: 66.67810625749955\n",
            "Epoch: 174 Loss: 66.66095902442422\n",
            "Epoch: 175 Loss: 66.64408859156988\n",
            "Epoch: 176 Loss: 66.62748875114305\n",
            "Epoch: 177 Loss: 66.61115347004642\n",
            "Epoch: 178 Loss: 66.59507688406445\n",
            "Epoch: 179 Loss: 66.57925329226903\n",
            "Epoch: 180 Loss: 66.56367715163553\n",
            "Epoch: 181 Loss: 66.54834307186076\n",
            "Epoch: 182 Loss: 66.53324581037452\n",
            "Epoch: 183 Loss: 66.5183802675369\n",
            "Epoch: 184 Loss: 66.50374148201303\n",
            "Epoch: 185 Loss: 66.4893246263191\n",
            "Epoch: 186 Loss: 66.47512500253166\n",
            "Epoch: 187 Loss: 66.46113803815399\n",
            "Epoch: 188 Loss: 66.44735928213342\n",
            "Epoch: 189 Loss: 66.43378440102306\n",
            "Epoch: 190 Loss: 66.4204091752821\n",
            "Epoch: 191 Loss: 66.40722949570974\n",
            "Epoch: 192 Loss: 66.39424136000656\n",
            "Epoch: 193 Loss: 66.381440869459\n",
            "Epoch: 194 Loss: 66.3688242257414\n",
            "Epoch: 195 Loss: 66.35638772783176\n",
            "Epoch: 196 Loss: 66.34412776903561\n",
            "Epoch: 197 Loss: 66.33204083411493\n",
            "Epoch: 198 Loss: 66.32012349651723\n",
            "Epoch: 199 Loss: 66.30837241570111\n",
            "Epoch: 200 Loss: 66.29678433455459\n",
            "Epoch: 201 Loss: 66.28535607690256\n",
            "Epoch: 202 Loss: 66.27408454509991\n",
            "Epoch: 203 Loss: 66.2629667177072\n",
            "Epoch: 204 Loss: 66.25199964724536\n",
            "Epoch: 205 Loss: 66.24118045802668\n",
            "Epoch: 206 Loss: 66.23050634405936\n",
            "Epoch: 207 Loss: 66.21997456702223\n",
            "Epoch: 208 Loss: 66.2095824543076\n",
            "Epoch: 209 Loss: 66.19932739712932\n",
            "Epoch: 210 Loss: 66.18920684869401\n",
            "Epoch: 211 Loss: 66.17921832243222\n",
            "Epoch: 212 Loss: 66.16935939028866\n",
            "Epoch: 213 Loss: 66.15962768106786\n",
            "Epoch: 214 Loss: 66.15002087883435\n",
            "Epoch: 215 Loss: 66.14053672136474\n",
            "Epoch: 216 Loss: 66.1311729986499\n",
            "Epoch: 217 Loss: 66.1219275514457\n",
            "Epoch: 218 Loss: 66.11279826986996\n",
            "Epoch: 219 Loss: 66.10378309204472\n",
            "Epoch: 220 Loss: 66.0948800027816\n",
            "Epoch: 221 Loss: 66.08608703230868\n",
            "Epoch: 222 Loss: 66.07740225503791\n",
            "Epoch: 223 Loss: 66.06882378837128\n",
            "Epoch: 224 Loss: 66.06034979154413\n",
            "Epoch: 225 Loss: 66.05197846450491\n",
            "Epoch: 226 Loss: 66.04370804682965\n",
            "Epoch: 227 Loss: 66.0355368166698\n",
            "Epoch: 228 Loss: 66.02746308973273\n",
            "Epoch: 229 Loss: 66.01948521829365\n",
            "Epoch: 230 Loss: 66.01160159023738\n",
            "Epoch: 231 Loss: 66.00381062812949\n",
            "Epoch: 232 Loss: 65.99611078831573\n",
            "Epoch: 233 Loss: 65.9885005600483\n",
            "Epoch: 234 Loss: 65.98097846463875\n",
            "Epoch: 235 Loss: 65.97354305463611\n",
            "Epoch: 236 Loss: 65.96619291302939\n",
            "Epoch: 237 Loss: 65.95892665247403\n",
            "Epoch: 238 Loss: 65.95174291454107\n",
            "Epoch: 239 Loss: 65.94464036898842\n",
            "Epoch: 240 Loss: 65.93761771305358\n",
            "Epoch: 241 Loss: 65.93067367076716\n",
            "Epoch: 242 Loss: 65.92380699228606\n",
            "Epoch: 243 Loss: 65.91701645324625\n",
            "Epoch: 244 Loss: 65.91030085413416\n",
            "Epoch: 245 Loss: 65.90365901967611\n",
            "Epoch: 246 Loss: 65.89708979824525\n",
            "Epoch: 247 Loss: 65.89059206128546\n",
            "Epoch: 248 Loss: 65.88416470275159\n",
            "Epoch: 249 Loss: 65.8778066385655\n",
            "Epoch: 250 Loss: 65.87151680608753\n",
            "Epoch: 251 Loss: 65.86529416360271\n",
            "Epoch: 252 Loss: 65.85913768982144\n",
            "Epoch: 253 Loss: 65.85304638339395\n",
            "Epoch: 254 Loss: 65.84701926243844\n",
            "Epoch: 255 Loss: 65.84105536408197\n",
            "Epoch: 256 Loss: 65.83515374401433\n",
            "Epoch: 257 Loss: 65.82931347605387\n",
            "Epoch: 258 Loss: 65.82353365172527\n",
            "Epoch: 259 Loss: 65.81781337984897\n",
            "Epoch: 260 Loss: 65.81215178614137\n",
            "Epoch: 261 Loss: 65.8065480128263\n",
            "Epoch: 262 Loss: 65.80100121825645\n",
            "Epoch: 263 Loss: 65.79551057654534\n",
            "Epoch: 264 Loss: 65.79007527720887\n",
            "Epoch: 265 Loss: 65.7846945248166\n",
            "Epoch: 266 Loss: 65.77936753865197\n",
            "Epoch: 267 Loss: 65.77409355238184\n",
            "Epoch: 268 Loss: 65.76887181373455\n",
            "Epoch: 269 Loss: 65.76370158418628\n",
            "Epoch: 270 Loss: 65.75858213865592\n",
            "Epoch: 271 Loss: 65.75351276520747\n",
            "Epoch: 272 Loss: 65.74849276476061\n",
            "Epoch: 273 Loss: 65.7435214508083\n",
            "Epoch: 274 Loss: 65.73859814914186\n",
            "Epoch: 275 Loss: 65.7337221975832\n",
            "Epoch: 276 Loss: 65.72889294572359\n",
            "Epoch: 277 Loss: 65.72410975466924\n",
            "Epoch: 278 Loss: 65.71937199679343\n",
            "Epoch: 279 Loss: 65.71467905549468\n",
            "Epoch: 280 Loss: 65.71003032496111\n",
            "Epoch: 281 Loss: 65.70542520994073\n",
            "Epoch: 282 Loss: 65.70086312551737\n",
            "Epoch: 283 Loss: 65.69634349689225\n",
            "Epoch: 284 Loss: 65.69186575917107\n",
            "Epoch: 285 Loss: 65.68742935715598\n",
            "Epoch: 286 Loss: 65.68303374514322\n",
            "Epoch: 287 Loss: 65.67867838672524\n",
            "Epoch: 288 Loss: 65.67436275459788\n",
            "Epoch: 289 Loss: 65.67008633037221\n",
            "Epoch: 290 Loss: 65.66584860439096\n",
            "Epoch: 291 Loss: 65.6616490755493\n",
            "Epoch: 292 Loss: 65.65748725112005\n",
            "Epoch: 293 Loss: 65.65336264658296\n",
            "Epoch: 294 Loss: 65.64927478545812\n",
            "Epoch: 295 Loss: 65.64522319914344\n",
            "Epoch: 296 Loss: 65.64120742675587\n",
            "Epoch: 297 Loss: 65.63722701497638\n",
            "Epoch: 298 Loss: 65.63328151789861\n",
            "Epoch: 299 Loss: 65.62937049688121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRZcVhTIjDhg",
        "colab_type": "text"
      },
      "source": [
        "A more computational level view of the feed forward pass\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/800/1*JHqzFok6Vz60HqoDf0OogQ.png\">\n",
        "\n",
        "**Training — Error, Backpropagation and Loss**\n",
        "\n",
        "**Error** — With y_pred, h and u, we proceed to calculate the error for this particular set of target and context words. This is done by summing up the difference between y_pred and each of the context words inw_c.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/560/1*pp5kV6uF7S0exTujskhbZw.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC-MDbKiWCB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1acf59e9-2334-4f6a-f0f8-d5769367d11b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-76-1e9a7753b0fa>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Error — With y_pred, h and u, we proceed to calculate the error for this particular set of target and context words. This is done by summing up the difference between y_pred and each of the context words inw_c.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuZfJ9gknGlN",
        "colab_type": "text"
      },
      "source": [
        "**Backpropagration**\n",
        "we need to alter the weights using the function backprop by passing in error EI, hidden layer h and vector for target word w_t.\n",
        "\n",
        "To update the weights, we multiply the weights to be adjusted (dl_dw1 and dl_dw2) with learning rate and then subtract it from the current weights (w1 and w2\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/560/1*ZWoH_NpUFGCPuHmtXUW5AA.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDf58tjUnQ0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}